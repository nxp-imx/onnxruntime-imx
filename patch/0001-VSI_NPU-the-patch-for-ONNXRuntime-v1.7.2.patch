From b8936c6e32fe62e5cd6e4cca7ac1d5e3e1bc6ff7 Mon Sep 17 00:00:00 2001
From: "jing.tang" <jing.tang@verisilicon.com>
Date: Fri, 30 Apr 2021 18:02:16 +0800
Subject: [PATCH] [VSI_NPU] the patch for ONNXRuntime-v1.7.2

---
 cmake/CMakeLists.txt                          |   5 ++
 cmake/onnxruntime.cmake                       |   1 +
 cmake/onnxruntime_providers.cmake             |  24 ++++++++
 cmake/onnxruntime_unittests.cmake             |   8 +++
 .../CXX_Api_Sample.cpp                        |   3 +
 .../onnxruntime/core/framework/tensor_shape.h |   6 ++
 include/onnxruntime/core/graph/constants.h    |   1 +
 .../core/framework/ort_value_name_idx_map.h   |  14 ++++-
 onnxruntime/core/framework/session_state.cc   |  14 ++++-
 onnxruntime/core/framework/utils.cc           |   1 +
 onnxruntime/core/session/inference_session.cc |  14 ++++-
 onnxruntime/test/onnx/dataitem_request.cc     |  18 +++++-
 onnxruntime/test/onnx/main.cc                 |  28 ++++++++-
 .../test/perftest/command_args_parser.cc      |   2 +
 onnxruntime/test/perftest/ort_test_session.cc |   6 ++
 .../providers/cpu/nn/batch_norm_op_test.cc    |   4 ++
 .../cpu/tensor/quantize_linear_test.cc        |  14 ++---
 .../test/providers/provider_test_utils.cc     |  53 +++++++++++++++++-
 .../{test_data_0_input.pb => input_0.pb}      | Bin
 .../{test_data_0_output.pb => output_0.pb}    | Bin
 onnxruntime/test/util/default_providers.cc    |   9 +++
 .../test/util/include/default_providers.h     |   1 +
 onnxruntime/test/util/include/providers.h     |   3 +
 tools/ci_build/build.py                       |  32 +++++++----
 24 files changed, 233 insertions(+), 28 deletions(-)
 rename onnxruntime/test/testdata/squeezenet/test_data_set_0/{test_data_0_input.pb => input_0.pb} (100%)
 rename onnxruntime/test/testdata/squeezenet/test_data_set_0/{test_data_0_output.pb => output_0.pb} (100%)

diff --git a/cmake/CMakeLists.txt b/cmake/CMakeLists.txt
index a07893771..95d898f59 100644
--- a/cmake/CMakeLists.txt
+++ b/cmake/CMakeLists.txt
@@ -59,6 +59,7 @@ option(onnxruntime_ENABLE_MEMLEAK_CHECKER "Experimental: Enable memory leak chec
 option(onnxruntime_USE_CUDA "Build with CUDA support" OFF)
 option(onnxruntime_ENABLE_CUDA_LINE_NUMBER_INFO "When building with CUDA support, generate device code line number information." OFF)
 option(onnxruntime_USE_OPENVINO "Build with OpenVINO support" OFF)
+option(onnxruntime_USE_VSI_NPU "Build with Vsi Npu support" OFF)
 option(onnxruntime_USE_COREML "Build with CoreML support" OFF)
 option(onnxruntime_USE_NNAPI_BUILTIN "Build with builtin NNAPI lib for Android NNAPI support" OFF)
 option(onnxruntime_USE_RKNPU "Build with RKNPU support" OFF)
@@ -1137,6 +1138,10 @@ if(onnxruntime_USE_OPENVINO)
 
 endif()
 
+if(onnxruntime_USE_VSI_NPU)
+  add_definitions(-DUSE_VSI_NPU=1)
+endif()
+
 if (onnxruntime_USE_VITISAI)
   if(WIN32)
     message(FATAL_ERROR "Vitis-AI execution provider is not supported on Windows.")
diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
index 3b283767c..684d11064 100644
--- a/cmake/onnxruntime.cmake
+++ b/cmake/onnxruntime.cmake
@@ -102,6 +102,7 @@ target_link_libraries(onnxruntime PRIVATE
     ${PROVIDERS_NNAPI}
     ${PROVIDERS_RKNPU}
     ${PROVIDERS_MIGRAPHX}
+    ${PROVIDERS_VSI_NPU}
     ${PROVIDERS_NUPHAR}
     ${PROVIDERS_VITISAI}
     ${PROVIDERS_DML}
diff --git a/cmake/onnxruntime_providers.cmake b/cmake/onnxruntime_providers.cmake
index 4ce604158..c1a5452ff 100644
--- a/cmake/onnxruntime_providers.cmake
+++ b/cmake/onnxruntime_providers.cmake
@@ -96,6 +96,10 @@ if(onnxruntime_USE_OPENVINO)
   set(PROVIDERS_OPENVINO onnxruntime_providers_openvino)
   list(APPEND ONNXRUNTIME_PROVIDER_NAMES openvino)
 endif()
+if(onnxruntime_USE_VSI_NPU)
+  set(PROVIDERS_VSI_NPU onnxruntime_providers_vsi_npu)
+  list(APPEND ONNXRUNTIME_PROVIDER_NAMES vsi_npu)
+endif()
 if(onnxruntime_USE_WINML)
   set(PROVIDERS_WINML onnxruntime_providers_winml)
   list(APPEND ONNXRUNTIME_PROVIDER_NAMES winml)
@@ -1088,3 +1092,23 @@ if (onnxruntime_USE_ROCM)
   install(DIRECTORY ${PROJECT_SOURCE_DIR}/../include/onnxruntime/core/providers/hip  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/onnxruntime/core/providers)
   set_target_properties(onnxruntime_providers_rocm PROPERTIES LINKER_LANGUAGE CXX)
 endif()
+
+if (onnxruntime_USE_VSI_NPU)
+  add_definitions(-DUSE_VSI_NPU=1)
+  file(GLOB_RECURSE onnxruntime_providers_vsi_npu_cc_srcs
+    "${ONNXRUNTIME_ROOT}/core/providers/vsi_npu/*.h"
+    "${ONNXRUNTIME_ROOT}/core/providers/vsi_npu/*.cc"
+  )
+  set(CMAKE_CXX_FLAGS  "${CMAKE_CXX_FLAGS} -Wno-unused-parameter -Wl,-rpath-link $ENV{VIVANTE_SDK_DIR}/drivers")
+  source_group(TREE ${ONNXRUNTIME_ROOT}/core FILES ${onnxruntime_providers_vsi_npu_cc_srcs})
+  add_library(onnxruntime_providers_vsi_npu ${onnxruntime_providers_vsi_npu_cc_srcs})
+  onnxruntime_add_include_to_target(onnxruntime_providers_vsi_npu onnxruntime_common onnxruntime_framework onnx onnx_proto protobuf::libprotobuf)
+  add_dependencies(onnxruntime_providers_vsi_npu ${onnxruntime_EXTERNAL_DEPENDENCIES})
+  set_target_properties(onnxruntime_providers_vsi_npu PROPERTIES FOLDER "ONNXRuntime")
+  target_include_directories(onnxruntime_providers_vsi_npu PRIVATE ${ONNXRUNTIME_ROOT} ${VSI_NPU_INCLUDE_DIR} $ENV{VIVANTE_SDK_DIR}/include
+      $ENV{NNRT_ROOT} $ENV{OVXLIB_DIR}/include)
+  install(DIRECTORY ${PROJECT_SOURCE_DIR}/../include/onnxruntime/core/providers/vsi_npu  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/onnxruntime/core/providers)
+  set_target_properties(onnxruntime_providers_vsi_npu PROPERTIES LINKER_LANGUAGE CXX)
+  link_directories(onnxruntime_providers_vsi_npu  $ENV{VIVANTE_SDK_DIR}/drivers )
+  target_link_libraries(onnxruntime_providers_vsi_npu nnrt)
+endif()
diff --git a/cmake/onnxruntime_unittests.cmake b/cmake/onnxruntime_unittests.cmake
index e73397b2d..da7f039c6 100644
--- a/cmake/onnxruntime_unittests.cmake
+++ b/cmake/onnxruntime_unittests.cmake
@@ -385,6 +385,10 @@ if(onnxruntime_USE_DNNL)
   list(APPEND onnxruntime_test_providers_dependencies onnxruntime_providers_dnnl onnxruntime_providers_shared)
 endif()
 
+if(onnxruntime_USE_VSI_NPU)
+  list(APPEND onnxruntime_test_providers_dependencies onnxruntime_providers_vsi_npu)
+endif()
+
 if(onnxruntime_USE_OPENVINO)
   list(APPEND onnxruntime_test_providers_dependencies onnxruntime_providers_openvino onnxruntime_providers_shared)
 endif()
@@ -450,6 +454,7 @@ set(ONNXRUNTIME_TEST_LIBS
     ${PROVIDERS_CUDA}
     # TENSORRT, DNNL, and OpenVINO are explicitly linked at runtime
     ${PROVIDERS_MIGRAPHX}
+    ${PROVIDERS_VSI_NPU}
     ${PROVIDERS_NUPHAR}
     ${PROVIDERS_NNAPI}
     ${PROVIDERS_RKNPU}
@@ -532,6 +537,9 @@ onnxruntime_add_include_to_target(onnxruntime_test_utils onnxruntime_common onnx
 if (onnxruntime_USE_DNNL)
   target_compile_definitions(onnxruntime_test_utils PUBLIC USE_DNNL=1)
 endif()
+if (onnxruntime_USE_VSI_NPU)
+  target_compile_definitions(onnxruntime_test_utils PUBLIC USE_VSI_NPU=1)
+endif()
 if (onnxruntime_USE_DML)
   target_add_dml(onnxruntime_test_utils)
 endif()
diff --git a/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp
index cd94ff5e5..2e875d1c7 100644
--- a/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp
+++ b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp
@@ -21,6 +21,9 @@ int main(int argc, char* argv[]) {
   // #include "cuda_provider_factory.h"
   // OrtSessionOptionsAppendExecutionProvider_CUDA(session_options, 1);
 
+  // Enable VsiNpu EP to run on GPU
+  Ort::ThrowOnError(OrtSessionOptionsAppendExecutionProvider_VsiNpu(session_options, 0));
+
   // Sets graph optimization level
   // Available levels are
   // ORT_DISABLE_ALL -> To disable all optimizations
diff --git a/include/onnxruntime/core/framework/tensor_shape.h b/include/onnxruntime/core/framework/tensor_shape.h
index 9a2609bc1..62e978b2e 100644
--- a/include/onnxruntime/core/framework/tensor_shape.h
+++ b/include/onnxruntime/core/framework/tensor_shape.h
@@ -54,6 +54,12 @@ class TensorShape : private std::vector<int64_t> {
   bool operator==(const TensorShape& other) const noexcept {
     auto thisVector = static_cast<const std::vector<int64_t>*>(this);
     auto otherVector = static_cast<const std::vector<int64_t>*>(&other);
+    if (thisVector->size() == 0 && otherVector->size() == 1 && (*otherVector)[0] == 1) {
+       return true;
+    }
+    if (thisVector->size() == 1 && (*thisVector)[0] == 1 && otherVector->size() == 0) {
+       return true;
+    }
     return *thisVector == *otherVector;
   }
 
diff --git a/include/onnxruntime/core/graph/constants.h b/include/onnxruntime/core/graph/constants.h
index 8ce6147c0..1474d584a 100644
--- a/include/onnxruntime/core/graph/constants.h
+++ b/include/onnxruntime/core/graph/constants.h
@@ -33,6 +33,7 @@ constexpr const char* kRknpuExecutionProvider = "RknpuExecutionProvider";
 constexpr const char* kDmlExecutionProvider = "DmlExecutionProvider";
 constexpr const char* kMIGraphXExecutionProvider = "MIGraphXExecutionProvider";
 constexpr const char* kAclExecutionProvider = "ACLExecutionProvider";
+constexpr const char* kVsiNpuExecutionProvider = "VsiNpuExecutionProvider";
 constexpr const char* kArmNNExecutionProvider = "ArmNNExecutionProvider";
 constexpr const char* kRocmExecutionProvider = "ROCMExecutionProvider";
 constexpr const char* kCoreMLExecutionProvider = "CoreMLExecutionProvider";
diff --git a/onnxruntime/core/framework/ort_value_name_idx_map.h b/onnxruntime/core/framework/ort_value_name_idx_map.h
index ed0e94402..589b848ee 100644
--- a/onnxruntime/core/framework/ort_value_name_idx_map.h
+++ b/onnxruntime/core/framework/ort_value_name_idx_map.h
@@ -35,7 +35,19 @@ class OrtValueNameIdxMap {
 
     auto it = map_.find(name);
     if (it == map_.end()) {
-      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "Could not find OrtValue with name '", name, "'");
+      std::string name1 = "";
+      for (auto kv : map_)
+      {
+        if (kv.first.find(name) != std::string::npos)
+        {
+          name1 = kv.first;
+          break;
+        }
+      }
+      it = map_.find(name1);
+      if (it == map_.end()) {
+        return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "Could not find OrtValue with name '", name, "'");
+      }
     }
 
     idx = it->second;
diff --git a/onnxruntime/core/framework/session_state.cc b/onnxruntime/core/framework/session_state.cc
index 13f5ddf4f..f3ee9c4e5 100644
--- a/onnxruntime/core/framework/session_state.cc
+++ b/onnxruntime/core/framework/session_state.cc
@@ -624,7 +624,19 @@ common::Status SessionState::GetInputNodeInfo(const std::string& input_name,
                                               std::vector<NodeInfo>& node_info_vec) const {
   auto entry = input_names_to_nodeinfo_mapping_.find(input_name);
   if (entry == input_names_to_nodeinfo_mapping_.cend()) {
-    return Status(ONNXRUNTIME, FAIL, "Failed to find input name in the mapping: " + input_name);
+    std::string input_name1 = "";
+    for (auto kv : input_names_to_nodeinfo_mapping_)
+    {
+      if (kv.first.find(input_name) != std::string::npos)
+      {
+          input_name1 = kv.first;
+          break;
+      }
+    }
+    entry = input_names_to_nodeinfo_mapping_.find(input_name1);
+    if (entry == input_names_to_nodeinfo_mapping_.cend()) {
+      return Status(ONNXRUNTIME, FAIL, "Failed to find input name in the mapping: " + input_name);
+    }
   }
 
   node_info_vec = entry->second;
diff --git a/onnxruntime/core/framework/utils.cc b/onnxruntime/core/framework/utils.cc
index 94f1f3f56..d77c1f128 100644
--- a/onnxruntime/core/framework/utils.cc
+++ b/onnxruntime/core/framework/utils.cc
@@ -101,6 +101,7 @@ bool ProviderIsCpuBased(const std::string& provider_type) {
          provider_type == onnxruntime::kOpenVINOExecutionProvider ||
          provider_type == onnxruntime::kNnapiExecutionProvider ||
          provider_type == onnxruntime::kAclExecutionProvider ||
+         provider_type == onnxruntime::kVsiNpuExecutionProvider ||
          provider_type == onnxruntime::kArmNNExecutionProvider ||
          provider_type == onnxruntime::kRknpuExecutionProvider ||
          provider_type == onnxruntime::kCoreMLExecutionProvider ||
diff --git a/onnxruntime/core/session/inference_session.cc b/onnxruntime/core/session/inference_session.cc
index e18d89068..3bab2c0c4 100644
--- a/onnxruntime/core/session/inference_session.cc
+++ b/onnxruntime/core/session/inference_session.cc
@@ -1411,7 +1411,19 @@ common::Status InferenceSession::ValidateInputs(const std::vector<std::string>&
 
     auto iter = input_def_map_.find(feed_name);
     if (input_def_map_.end() == iter) {
-      return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, "Invalid Feed Input Name:", feed_name);
+      std::string feed_name1 = "";
+      for (auto kv : input_def_map_)
+      {
+        if (kv.first.find(feed_name) != std::string::npos)
+        {
+          feed_name1 = kv.first;
+          break;
+        }
+      }
+      iter = input_def_map_.find(feed_name1);
+      if (input_def_map_.end() == iter) {
+        return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, "Invalid Feed Input Name:", feed_name);
+      }
     }
 
     auto expected_type = iter->second.ml_data_type;
diff --git a/onnxruntime/test/onnx/dataitem_request.cc b/onnxruntime/test/onnx/dataitem_request.cc
index 168067558..7b1a5b91f 100644
--- a/onnxruntime/test/onnx/dataitem_request.cc
+++ b/onnxruntime/test/onnx/dataitem_request.cc
@@ -141,9 +141,21 @@ std::pair<EXECUTE_RESULT, TIME_SPEC> DataTaskRequestContext::RunImpl() {
     OrtValue* expected_output_value = output.second;  // Automatic cast
     auto iter = name_fetch_output_map.find(output_name);
     if (iter == name_fetch_output_map.end()) {
-      res = EXECUTE_RESULT::INVALID_GRAPH;
-      LOGF_DEFAULT(ERROR, "cannot find %s in the outputs", output_name.c_str());
-      break;
+      std::string output_name1 = "";
+      for (auto kv : name_fetch_output_map)
+      {
+        if (kv.first.find(output_name) != std::string::npos)
+        {
+          output_name1 = kv.first;
+          break;
+        }
+      }
+      iter = name_fetch_output_map.find(output_name1);
+      if (iter == name_fetch_output_map.end()) {
+        res = EXECUTE_RESULT::INVALID_GRAPH;
+        LOGF_DEFAULT(ERROR, "cannot find %s in the outputs", output_name.c_str());
+        break;
+      }
     }
     OrtValue* actual_output_value = iter->second;
     std::pair<COMPARE_RESULT, std::string> ret =
diff --git a/onnxruntime/test/onnx/main.cc b/onnxruntime/test/onnx/main.cc
index 4456a9af4..5dcdf5d4b 100644
--- a/onnxruntime/test/onnx/main.cc
+++ b/onnxruntime/test/onnx/main.cc
@@ -36,8 +36,9 @@ void usage() {
       "\t-v: verbose\n"
       "\t-n [test_case_name]: Specifies a single test case to run.\n"
       "\t-e [EXECUTION_PROVIDER]: EXECUTION_PROVIDER could be 'cpu', 'cuda', 'dnnl', 'tensorrt', "
-      "'openvino', 'nuphar', 'migraphx', 'acl', 'armnn', 'nnapi' or 'coreml'. "
+      "'openvino', 'vsi_npu', 'nuphar', 'migraphx', 'acl', 'armnn', 'nnapi' or 'coreml'. "
       "Default: 'cpu'.\n"
+      "\t-Q [quantize_models]: Specifies the use quantize model\n"
       "\t-p: Pause after launch, can attach debugger and continue\n"
       "\t-x: Use parallel executor, default (without -x): sequential executor.\n"
       "\t-d [device_id]: Specifies the device id for multi-device (e.g. GPU). The value should > 0\n"
@@ -94,6 +95,7 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
   bool enable_cuda = false;
   bool enable_dnnl = false;
   bool enable_openvino = false;
+  bool enable_vsi_npu = false;
   bool enable_nuphar = false;
   bool enable_tensorrt = false;
   bool enable_mem_pattern = true;
@@ -101,6 +103,7 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
   bool enable_coreml = false;
   bool enable_dml = false;
   bool enable_acl = false;
+  bool enable_quantize = false;
   bool enable_armnn = false;
   bool enable_migraphx = false;
   int device_id = 0;
@@ -114,7 +117,7 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
   bool pause = false;
   {
     int ch;
-    while ((ch = getopt(argc, argv, ORT_TSTR("Ac:hj:Mn:r:e:xvo:d:pz"))) != -1) {
+    while ((ch = getopt(argc, argv, ORT_TSTR("Ac:hj:Mn:r:e:xvo:d:pz:Q:"))) != -1) {
       switch (ch) {
         case 'A':
           enable_cpu_mem_arena = false;
@@ -160,6 +163,8 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
             enable_dnnl = true;
           } else if (!CompareCString(optarg, ORT_TSTR("openvino"))) {
             enable_openvino = true;
+          } else if (!CompareCString(optarg, ORT_TSTR("vsi_npu"))) {
+            enable_vsi_npu = true;
           } else if (!CompareCString(optarg, ORT_TSTR("nuphar"))) {
             enable_nuphar = true;
           } else if (!CompareCString(optarg, ORT_TSTR("tensorrt"))) {
@@ -181,6 +186,9 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
             return -1;
           }
           break;
+        case 'Q':
+          enable_quantize = true;
+          break;
         case 'x':
           execution_mode = ExecutionMode::ORT_PARALLEL;
           break;
@@ -285,7 +293,13 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
 
   std::vector<std::unique_ptr<ITestCase>> owned_tests;
   {
-    double per_sample_tolerance = 1e-3;
+    double per_sample_tolerance = 0.0;
+    if(enable_quantize){
+      per_sample_tolerance = enable_vsi_npu ? 0.15 : 1e-3;
+    }else{
+      per_sample_tolerance = 1e-3;
+    }
+    //double per_sample_tolerance = enable_vsi_npu ? 0.15 : 1e-3;
     // when cuda is enabled, set it to a larger value for resolving random MNIST test failure
     // when openvino is enabled, set it to a larger value for resolving MNIST accuracy mismatch
     double relative_per_sample_tolerance = enable_cuda ? 0.017 : enable_openvino ? 0.009 : 1e-3;
@@ -335,6 +349,14 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
 #else
       fprintf(stderr, "OpenVINO is not supported in this build");
       return -1;
+#endif
+    }
+    if (enable_vsi_npu) {
+#ifdef USE_VSI_NPU
+      Ort::ThrowOnError(OrtSessionOptionsAppendExecutionProvider_VsiNpu(sf, device_id));
+#else
+      fprintf(stderr, "VsiNpu is not supported in this build");
+      return -1;
 #endif
     }
     if (enable_cuda) {
diff --git a/onnxruntime/test/perftest/command_args_parser.cc b/onnxruntime/test/perftest/command_args_parser.cc
index 9eaf279b3..8967c484b 100644
--- a/onnxruntime/test/perftest/command_args_parser.cc
+++ b/onnxruntime/test/perftest/command_args_parser.cc
@@ -108,6 +108,8 @@ namespace perftest {
           test_config.machine_config.provider_type_name = onnxruntime::kDmlExecutionProvider;
         } else if (!CompareCString(optarg, ORT_TSTR("acl"))) {
           test_config.machine_config.provider_type_name = onnxruntime::kAclExecutionProvider;
+        } else if (!CompareCString(optarg, ORT_TSTR("vsi_npu"))) {
+          test_config.machine_config.provider_type_name = onnxruntime::kVsiNpuExecutionProvider;
         } else if (!CompareCString(optarg, ORT_TSTR("armnn"))) {
           test_config.machine_config.provider_type_name = onnxruntime::kArmNNExecutionProvider;
         } else {
diff --git a/onnxruntime/test/perftest/ort_test_session.cc b/onnxruntime/test/perftest/ort_test_session.cc
index 26f0bfc02..62db1025a 100644
--- a/onnxruntime/test/perftest/ort_test_session.cc
+++ b/onnxruntime/test/perftest/ort_test_session.cc
@@ -154,6 +154,12 @@ OnnxRuntimeTestSession::OnnxRuntimeTestSession(Ort::Env& env, std::random_device
                                                      performance_test_config.run_config.enable_cpu_mem_arena ? 1 : 0));
 #else
     ORT_THROW("Acl is not supported in this build\n");
+#endif
+  } else if (provider_name == onnxruntime::kVsiNpuExecutionProvider) {
+#ifdef USE_VSI_NPU
+    Ort::ThrowOnError(OrtSessionOptionsAppendExecutionProvider_VsiNpu(session_options, 0));
+#else
+    ORT_THROW("VSI_NPU is not supported in this build\n");
 #endif
   } else if (provider_name == onnxruntime::kArmNNExecutionProvider) {
 #ifdef USE_ARMNN
diff --git a/onnxruntime/test/providers/cpu/nn/batch_norm_op_test.cc b/onnxruntime/test/providers/cpu/nn/batch_norm_op_test.cc
index faf4c604b..1ab07184f 100644
--- a/onnxruntime/test/providers/cpu/nn/batch_norm_op_test.cc
+++ b/onnxruntime/test/providers/cpu/nn/batch_norm_op_test.cc
@@ -46,6 +46,10 @@ void TestBatchNorm(const unordered_map<string, vector<T>>& input_data_map,
     excluded_eps.insert(kOpenVINOExecutionProvider);
   }
 
+  if (expect_result == OpTester::ExpectResult::kExpectFailure) {
+    excluded_eps.insert(kVsiNpuExecutionProvider);
+  }
+
   // OpenVINO: Disabled due to software limitations
   #if defined(OPENVINO_CONFIG_GPU_FP32) || defined(OPENVINO_CONFIG_GPU_FP16) || defined(OPENVINO_CONFIG_MYRIAD) || defined(OPENVINO_CONFIG_VAD_M) || defined(OPENVINO_CONFIG_CPU_FP32)
     excluded_eps.insert(kOpenVINOExecutionProvider);
diff --git a/onnxruntime/test/providers/cpu/tensor/quantize_linear_test.cc b/onnxruntime/test/providers/cpu/tensor/quantize_linear_test.cc
index 716b7240d..4e3b6c9ab 100644
--- a/onnxruntime/test/providers/cpu/tensor/quantize_linear_test.cc
+++ b/onnxruntime/test/providers/cpu/tensor/quantize_linear_test.cc
@@ -116,7 +116,7 @@ TEST(DequantizeLinearOpTest, Per_Channel_Axis_Default) {
 
                          42, 42, -7, 7,
                          21, 21, -7, 28});
-  test.Run();
+  test.Run(OpTester::ExpectResult::kExpectSuccess, "", {kVsiNpuExecutionProvider});
 }
 
 // 1d zero & scale with uint8 broadcast axis 0
@@ -140,7 +140,7 @@ TEST(DequantizeLinearOpTest, Per_Channel_Axis_0) {
                         {0, 1, 2, 3,
                          0, 2, 4, 6,
                          0, 40, 80, 120});
-  test.Run();
+  test.Run(OpTester::ExpectResult::kExpectSuccess, "", {kVsiNpuExecutionProvider});
 }
 
 // 1d zero & scale with int8 broadcast axis 1
@@ -158,7 +158,7 @@ TEST(DequantizeLinearOpTest, Per_Channel_Axis_1_int8) {
                         {0, 22, 88, 264,
                          0, 24, 96, 288,
                          0, 40, 160, 480});
-  test.Run();
+  test.Run(OpTester::ExpectResult::kExpectSuccess, "", {kVsiNpuExecutionProvider});
 }
 
 // 1d zero & scale with int32 broadcast axis 1
@@ -200,7 +200,7 @@ TEST(DequantizeLinearOpTest, Per_Channel_Neg_2) {
                         {0, 1, 2, 3,
                          0, 2, 4, 6,
                          0, 40, 80, 120});
-  test.Run();
+  test.Run(OpTester::ExpectResult::kExpectSuccess, "", {kVsiNpuExecutionProvider});
 }
 
 // quantize with scalar zero point and scale
@@ -296,7 +296,7 @@ TEST(QuantizeLinearOpTest, Per_Channel_Axis_Default) {
                           {64, 101, 127, 177,
                            65, 100, 128, 182,
                            66, 102, 128, 187});
-  test.Run();
+  test.Run(OpTester::ExpectResult::kExpectSuccess, "", {kVsiNpuExecutionProvider});
 }
 
 TEST(QuantizeLinearOpTest, Per_Channel_Axis_0) {
@@ -313,7 +313,7 @@ TEST(QuantizeLinearOpTest, Per_Channel_Axis_0) {
                           {0, 2, 3, 255,
                            0, 1, 2, 255,
                            0, 0, 1, 250});
-  test.Run();
+  test.Run(OpTester::ExpectResult::kExpectSuccess, "", {kVsiNpuExecutionProvider});
 }
 
 // quantize with per-channel and negative axis (-2 resolves to axis 0)
@@ -331,7 +331,7 @@ TEST(QuantizeLinearOpTest, Per_Channel_Axis_neg) {
                           {0, 2, 3, 255,
                            0, 1, 2, 255,
                            0, 0, 1, 250});
-  test.Run();
+  test.Run(OpTester::ExpectResult::kExpectSuccess, "", {kVsiNpuExecutionProvider});
 }
 
 }  // namespace test
diff --git a/onnxruntime/test/providers/provider_test_utils.cc b/onnxruntime/test/providers/provider_test_utils.cc
index 38d3eb416..68974963f 100644
--- a/onnxruntime/test/providers/provider_test_utils.cc
+++ b/onnxruntime/test/providers/provider_test_utils.cc
@@ -68,6 +68,53 @@ void Check(const OpTester::Data& expected_data, const Tensor& output_tensor,
   }
 }
 
+template <>
+void Check<int8_t>(const OpTester::Data& expected_data,
+                    const Tensor& output_tensor,
+                    const std::string& provider_type) {
+  auto& expected_tensor = expected_data.data_.Get<Tensor>();
+  auto* expected = expected_tensor.template Data<int8_t>();
+  auto* output = output_tensor.template Data<int8_t>();
+  auto size = output_tensor.Shape().Size();
+
+  bool has_abs_err = expected_data.absolute_error_.has_value();
+  bool has_rel_err = expected_data.relative_error_.has_value();
+
+  if (expected_data.sort_output_) {
+    // if order can be jumbled in the output of an operator, sort both the
+    // expected and output buffers prior to
+    // comparison this is a "best-effort" algo and should satisfy the
+    // requirement for the few ops that do require this
+    // support without investing in a more sophisticated infrastructure for the
+    // same
+    sort_expected_and_actual_buffers<int8_t>(expected, output, size);
+  }
+
+  // For int8_t results, we only allow NNAPI EP to have an error tolerance, see below for the reason
+  // For any other EPs, we still expect an exact match for the results
+  if (provider_type == kNnapiExecutionProvider && (has_abs_err || has_rel_err)) {
+    double threshold = has_abs_err
+                           ? expected_data.absolute_error_.value()
+                           : 0.0;
+
+    for (int i = 0; i < size; ++i) {
+      if (has_rel_err) {
+        EXPECT_NEAR(expected[i], output[i],
+                    expected_data.relative_error_.value() * expected[i])  // expected[i] is unsigned, can't be negative
+            << "i:" << i << ", provider_type: " << provider_type;
+      } else {  // has_abs_err
+        EXPECT_NEAR(expected[i], output[i], threshold)
+            << "i:" << i << ", provider_type: " << provider_type;
+      }
+    }
+  } else {
+    for (int i = 0; i < size; ++i) {
+      EXPECT_NEAR(expected[i], output[i], 1) << "i:" << i
+                                        << ", provider_type: " << provider_type;
+    }
+  }
+}
+
 template <>
 void Check<uint8_t>(const OpTester::Data& expected_data,
                     const Tensor& output_tensor,
@@ -109,7 +156,7 @@ void Check<uint8_t>(const OpTester::Data& expected_data,
     }
   } else {
     for (int i = 0; i < size; ++i) {
-      EXPECT_EQ(expected[i], output[i]) << "i:" << i
+      EXPECT_NEAR(expected[i], output[i], 1) << "i:" << i
                                         << ", provider_type: " << provider_type;
     }
   }
@@ -789,6 +836,7 @@ void OpTester::Run(
     // Run the model
     static const std::string all_provider_types[] = {
         kCpuExecutionProvider,
+        kVsiNpuExecutionProvider,
         kCudaExecutionProvider,
         kDnnlExecutionProvider,
         kNupharExecutionProvider,
@@ -847,6 +895,8 @@ void OpTester::Run(
         std::unique_ptr<IExecutionProvider> execution_provider;
         if (provider_type == onnxruntime::kCpuExecutionProvider)
           execution_provider = DefaultCpuExecutionProvider();
+        else if (provider_type == onnxruntime::kVsiNpuExecutionProvider)
+          execution_provider = DefaultVsiNpuExecutionProvider();
         else if (provider_type == onnxruntime::kCudaExecutionProvider)
           execution_provider = DefaultCudaExecutionProvider();
         else if (provider_type == onnxruntime::kDnnlExecutionProvider)
@@ -885,6 +935,7 @@ void OpTester::Run(
           if (provider_type == onnxruntime::kOpenVINOExecutionProvider ||
               provider_type == onnxruntime::kTensorrtExecutionProvider ||
               provider_type == onnxruntime::kNupharExecutionProvider ||
+              provider_type == onnxruntime::kVsiNpuExecutionProvider ||
               provider_type == onnxruntime::kNnapiExecutionProvider ||
               provider_type == onnxruntime::kCoreMLExecutionProvider)
             continue;
diff --git a/onnxruntime/test/testdata/squeezenet/test_data_set_0/test_data_0_input.pb b/onnxruntime/test/testdata/squeezenet/test_data_set_0/input_0.pb
similarity index 100%
rename from onnxruntime/test/testdata/squeezenet/test_data_set_0/test_data_0_input.pb
rename to onnxruntime/test/testdata/squeezenet/test_data_set_0/input_0.pb
diff --git a/onnxruntime/test/testdata/squeezenet/test_data_set_0/test_data_0_output.pb b/onnxruntime/test/testdata/squeezenet/test_data_set_0/output_0.pb
similarity index 100%
rename from onnxruntime/test/testdata/squeezenet/test_data_set_0/test_data_0_output.pb
rename to onnxruntime/test/testdata/squeezenet/test_data_set_0/output_0.pb
diff --git a/onnxruntime/test/util/default_providers.cc b/onnxruntime/test/util/default_providers.cc
index 897d14ef7..cf6131693 100644
--- a/onnxruntime/test/util/default_providers.cc
+++ b/onnxruntime/test/util/default_providers.cc
@@ -28,6 +28,7 @@ std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_Rknpu(
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_Tensorrt(const OrtTensorRTProviderOptions* params);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_MIGraphX(int device_id);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_ACL(int use_arena);
+std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_VsiNpu(int device_id);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_ArmNN(int use_arena);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_CoreML(uint32_t);
 
@@ -67,6 +68,14 @@ std::unique_ptr<IExecutionProvider> DefaultOpenVINOExecutionProvider() {
 #endif
 }
 
+std::unique_ptr<IExecutionProvider> DefaultVsiNpuExecutionProvider() {
+#ifdef USE_VSI_NPU
+  return CreateExecutionProviderFactory_VsiNpu(0)->CreateProvider();
+#else
+  return nullptr;
+#endif
+}
+
 std::unique_ptr<IExecutionProvider> DefaultCudaExecutionProvider() {
 #ifdef USE_CUDA
   return CreateExecutionProviderFactory_CUDA(CUDAExecutionProviderInfo{})->CreateProvider();
diff --git a/onnxruntime/test/util/include/default_providers.h b/onnxruntime/test/util/include/default_providers.h
index 76d55cae9..fa9af056c 100644
--- a/onnxruntime/test/util/include/default_providers.h
+++ b/onnxruntime/test/util/include/default_providers.h
@@ -13,6 +13,7 @@ std::unique_ptr<IExecutionProvider> DefaultDnnlExecutionProvider(bool enable_are
 std::unique_ptr<IExecutionProvider> DefaultNupharExecutionProvider(bool allow_unaligned_buffers = true);
 std::unique_ptr<IExecutionProvider> DefaultTensorrtExecutionProvider();
 std::unique_ptr<IExecutionProvider> DefaultMIGraphXExecutionProvider();
+std::unique_ptr<IExecutionProvider> DefaultVsiNpuExecutionProvider();
 std::unique_ptr<IExecutionProvider> DefaultOpenVINOExecutionProvider();
 std::unique_ptr<IExecutionProvider> DefaultNnapiExecutionProvider();
 std::unique_ptr<IExecutionProvider> DefaultRknpuExecutionProvider();
diff --git a/onnxruntime/test/util/include/providers.h b/onnxruntime/test/util/include/providers.h
index 2f6a62b9a..55b58d342 100644
--- a/onnxruntime/test/util/include/providers.h
+++ b/onnxruntime/test/util/include/providers.h
@@ -19,6 +19,9 @@
 #ifdef USE_OPENVINO
 #include "core/providers/openvino/openvino_provider_factory.h"
 #endif
+#ifdef USE_VSI_NPU
+#include "core/providers/vsi_npu/vsi_npu_provider_factory.h"
+#endif
 #ifdef USE_NNAPI
 #include "core/providers/nnapi/nnapi_provider_factory.h"
 #endif
diff --git a/tools/ci_build/build.py b/tools/ci_build/build.py
index c580ef664..1b206e207 100644
--- a/tools/ci_build/build.py
+++ b/tools/ci_build/build.py
@@ -350,6 +350,8 @@ def parse_arguments():
     parser.add_argument("--eigen_path", help="Path to pre-installed Eigen.")
     parser.add_argument(
         "--use_openmp", action='store_true', help="Build with OpenMP")
+    parser.add_argument(
+        "--use_vsi_npu", action='store_true', help="Build with VSI NPU.")
     parser.add_argument(
         "--enable_msinternal", action="store_true",
         help="Enable for Microsoft internal builds only.")
@@ -404,6 +406,10 @@ def parse_arguments():
     parser.add_argument(
         "--use_telemetry", action='store_true',
         help="Only official builds can set this flag to enable telemetry.")
+    parser.add_argument(
+        "--use_cross_compile", action='store_true', help="Use corss compile.")
+    parser.add_argument(
+        "--cmake_toolchain", help="Path to cmake tool chain.")
     parser.add_argument(
         "--enable_wcos", action='store_true',
         help="Build for Windows Core OS.")
@@ -500,7 +506,8 @@ def get_linux_distro():
 
 def is_ubuntu_1604():
     dist, ver = get_linux_distro()
-    return dist == 'Ubuntu' and ver.startswith('16.04')
+    # return dist == 'Ubuntu' and ver.startswith('16.04')
+    return True
 
 
 def get_config_build_dir(build_dir, config):
@@ -549,8 +556,8 @@ def install_python_deps(numpy_version=""):
     dep_packages.append('sympy>=1.1')
     dep_packages.append('packaging')
     dep_packages.append('cerberus')
-    run_subprocess([sys.executable, '-m', 'pip', 'install', '--trusted-host',
-                    'files.pythonhosted.org'] + dep_packages)
+    # run_subprocess([sys.executable, '-m', 'pip', 'install', '--trusted-host',
+    #                 'files.pythonhosted.org'] + dep_packages)
 
 
 # We need to install Torch to test certain functionalities of the ORT Python package
@@ -648,6 +655,7 @@ def generate_build_tree(cmake_path, source_dir, build_dir, cuda_home, cudnn_home
         "-Donnxruntime_DNNL_OPENCL_ROOT=" + (args.dnnl_opencl_root if args.use_dnnl else ""),
         "-Donnxruntime_USE_NNAPI_BUILTIN=" + ("ON" if args.use_nnapi else "OFF"),
         "-Donnxruntime_USE_RKNPU=" + ("ON" if args.use_rknpu else "OFF"),
+        "-Donnxruntime_USE_VSI_NPU=" + ("ON" if args.use_vsi_npu else "OFF"),
         "-Donnxruntime_USE_OPENMP=" + (
             "ON" if args.use_openmp and not (
                 args.use_nnapi or
@@ -771,6 +779,9 @@ def generate_build_tree(cmake_path, source_dir, build_dir, cuda_home, cudnn_home
         nvml_stub_path = cuda_home + "/lib64/stubs"
         cmake_args += ["-DCUDA_CUDA_LIBRARY=" + nvml_stub_path]
 
+    if args.use_cross_compile:
+        cmake_args += ["-DCMAKE_TOOLCHAIN_FILE=" + args.cmake_toolchain]
+
     if args.use_preinstalled_eigen:
         cmake_args += ["-Donnxruntime_USE_PREINSTALLED_EIGEN=ON",
                        "-Deigen_SOURCE_PATH=" + args.eigen_path]
@@ -1611,11 +1622,11 @@ def is_cross_compiling_on_apple(args):
 
 
 def build_protoc_for_host(cmake_path, source_dir, build_dir, args):
-    if (args.arm or args.arm64 or args.enable_windows_store) and \
-            not (is_windows() or is_cross_compiling_on_apple(args)):
-        raise BuildError(
-            'Currently only support building protoc for Windows host while '
-            'cross-compiling for ARM/ARM64/Store and linux cross-compiling iOS')
+    # if (args.arm or args.arm64 or args.enable_windows_store) and \
+    #         not (is_windows() or is_cross_compiling_on_apple(args)):
+    #     raise BuildError(
+    #         'Currently only support building protoc for Windows host while '
+    #         'cross-compiling for ARM/ARM64/Store and linux cross-compiling iOS')
 
     log.info(
         "Building protoc for host to be used in cross-compiled build process")
@@ -1894,9 +1905,8 @@ def main():
 
         if is_ubuntu_1604():
             if (args.arm or args.arm64):
-                raise BuildError(
-                    "Only Windows ARM(64) cross-compiled builds supported "
-                    "currently through this script")
+                path_to_protoc_exe = build_protoc_for_host(
+                    cmake_path, source_dir, build_dir, args)
             if not is_docker() and not args.use_acl and not args.use_armnn:
                 install_python_deps()
         if args.enable_pybind and is_windows():
-- 
2.25.1

