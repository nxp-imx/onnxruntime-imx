From 9cfade9eb33d6908c8dc82bfe8090a44a164b9aa Mon Sep 17 00:00:00 2001
From: "jing.tang" <jing.tang@verisilicon.com>
Date: Fri, 11 Dec 2020 15:44:42 +0800
Subject: [PATCH] [VSI_NPU] the patch for ONNXRuntime v1.5.3

---
 cmake/CMakeLists.txt                          |   5 +++
 cmake/onnxruntime.cmake                       |   1 +
 cmake/onnxruntime_providers.cmake             |  24 ++++++++++
 cmake/onnxruntime_unittests.cmake             |   8 ++++
 .../CXX_Api_Sample.cpp                        |   4 ++
 .../onnxruntime/core/framework/tensor_shape.h |   6 +++
 include/onnxruntime/core/graph/constants.h    |   1 +
 .../core/framework/ort_value_name_idx_map.h   |  14 +++++-
 onnxruntime/core/framework/session_state.cc   |  14 +++++-
 .../core/optimizer/transformer_memcpy.cc      |   1 +
 onnxruntime/core/session/inference_session.cc |  14 +++++-
 onnxruntime/test/onnx/main.cc                 |  28 ++++++++++--
 onnxruntime/test/onnx/runner.cc               |  18 ++++++--
 .../test/perftest/command_args_parser.cc      |   2 +
 onnxruntime/test/perftest/ort_test_session.cc |   6 +++
 .../providers/cpu/nn/batch_norm_op_test.cc    |   3 ++
 .../test/providers/provider_test_utils.cc     |  42 ++++++++++++++++++
 .../{test_data_0_input.pb => input_0.pb}      | Bin
 .../{test_data_0_output.pb => output_0.pb}    | Bin
 onnxruntime/test/util/default_providers.cc    |   9 ++++
 .../test/util/include/default_providers.h     |   1 +
 onnxruntime/test/util/include/providers.h     |   3 ++
 tools/ci_build/build.py                       |  33 +++++++++-----
 23 files changed, 217 insertions(+), 20 deletions(-)
 rename onnxruntime/test/testdata/squeezenet/test_data_set_0/{test_data_0_input.pb => input_0.pb} (100%)
 rename onnxruntime/test/testdata/squeezenet/test_data_set_0/{test_data_0_output.pb => output_0.pb} (100%)

diff --git a/cmake/CMakeLists.txt b/cmake/CMakeLists.txt
index 853651ac5..be5fb595c 100644
--- a/cmake/CMakeLists.txt
+++ b/cmake/CMakeLists.txt
@@ -57,6 +57,7 @@ option(onnxruntime_ENABLE_PYTHON "Enable python buildings" OFF)
 option(onnxruntime_ENABLE_MEMLEAK_CHECKER "Experimental: Enable memory leak checker in Windows debug build" OFF)
 option(onnxruntime_USE_CUDA "Build with CUDA support" OFF)
 option(onnxruntime_USE_OPENVINO "Build with OpenVINO support" OFF)
+option(onnxruntime_USE_VSI_NPU "Build with Vsi Npu support" OFF)
 option(onnxruntime_USE_EIGEN_FOR_BLAS "Use eign for blas" ON)
 option(onnxruntime_USE_NNAPI_BUILTIN "Build with builtin NNAPI lib for Android NNAPI support" OFF)
 option(onnxruntime_USE_RKNPU "Build with RKNPU support" OFF)
@@ -974,6 +975,10 @@ if(onnxruntime_USE_OPENVINO)
 
 endif()
 
+if(onnxruntime_USE_VSI_NPU)
+  add_definitions(-DUSE_VSI_NPU=1)
+endif()
+
 if (onnxruntime_USE_VITISAI)
   if(WIN32)
     message(FATAL_ERROR "Vitis-AI execution provider is not supported on Windows.")
diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
index 512367174..fcc9a75f4 100644
--- a/cmake/onnxruntime.cmake
+++ b/cmake/onnxruntime.cmake
@@ -102,6 +102,7 @@ target_link_libraries(onnxruntime PRIVATE
     ${PROVIDERS_RKNPU}
     ${PROVIDERS_MIGRAPHX}
     ${PROVIDERS_OPENVINO}
+    ${PROVIDERS_VSI_NPU}
     ${PROVIDERS_NUPHAR}
     ${PROVIDERS_VITISAI}
     ${PROVIDERS_DML}
diff --git a/cmake/onnxruntime_providers.cmake b/cmake/onnxruntime_providers.cmake
index 6ea104a8f..1764aa9b8 100644
--- a/cmake/onnxruntime_providers.cmake
+++ b/cmake/onnxruntime_providers.cmake
@@ -75,6 +75,10 @@ if(onnxruntime_USE_OPENVINO)
   set(PROVIDERS_OPENVINO onnxruntime_providers_openvino)
   list(APPEND ONNXRUNTIME_PROVIDER_NAMES openvino)
 endif()
+if(onnxruntime_USE_VSI_NPU)
+  set(PROVIDERS_VSI_NPU onnxruntime_providers_vsi_npu)
+  list(APPEND ONNXRUNTIME_PROVIDER_NAMES vsi_npu)
+endif()
 if(onnxruntime_USE_WINML)
   set(PROVIDERS_WINML onnxruntime_providers_winml)
   list(APPEND ONNXRUNTIME_PROVIDER_NAMES winml)
@@ -774,3 +778,23 @@ if (onnxruntime_USE_ARMNN)
   install(DIRECTORY ${PROJECT_SOURCE_DIR}/../include/onnxruntime/core/providers/armnn  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/onnxruntime/core/providers)
   set_target_properties(onnxruntime_providers_armnn PROPERTIES LINKER_LANGUAGE CXX)
 endif()
+
+if (onnxruntime_USE_VSI_NPU)
+  add_definitions(-DUSE_VSI_NPU=1)
+  file(GLOB_RECURSE onnxruntime_providers_vsi_npu_cc_srcs
+    "${ONNXRUNTIME_ROOT}/core/providers/vsi_npu/*.h"
+    "${ONNXRUNTIME_ROOT}/core/providers/vsi_npu/*.cc"
+  )
+  set(CMAKE_CXX_FLAGS  "${CMAKE_CXX_FLAGS} -Wno-unused-parameter -Wl,-rpath-link $ENV{VIVANTE_SDK_DIR}/drivers")
+  source_group(TREE ${ONNXRUNTIME_ROOT}/core FILES ${onnxruntime_providers_vsi_npu_cc_srcs})
+  add_library(onnxruntime_providers_vsi_npu ${onnxruntime_providers_vsi_npu_cc_srcs})
+  onnxruntime_add_include_to_target(onnxruntime_providers_vsi_npu onnxruntime_common onnxruntime_framework onnx onnx_proto protobuf::libprotobuf)
+  add_dependencies(onnxruntime_providers_vsi_npu ${onnxruntime_EXTERNAL_DEPENDENCIES})
+  set_target_properties(onnxruntime_providers_vsi_npu PROPERTIES FOLDER "ONNXRuntime")
+  target_include_directories(onnxruntime_providers_vsi_npu PRIVATE ${ONNXRUNTIME_ROOT} ${VSI_NPU_INCLUDE_DIR} $ENV{VIVANTE_SDK_DIR}/include
+      $ENV{NNRT_ROOT} $ENV{OVXLIB_DIR}/include)
+  install(DIRECTORY ${PROJECT_SOURCE_DIR}/../include/onnxruntime/core/providers/vsi_npu  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/onnxruntime/core/providers)
+  set_target_properties(onnxruntime_providers_vsi_npu PROPERTIES LINKER_LANGUAGE CXX)
+  link_directories(onnxruntime_providers_vsi_npu  $ENV{VIVANTE_SDK_DIR}/drivers )
+  target_link_libraries(onnxruntime_providers_vsi_npu nnrt)
+endif()
\ No newline at end of file
diff --git a/cmake/onnxruntime_unittests.cmake b/cmake/onnxruntime_unittests.cmake
index 20349383a..8d7554246 100644
--- a/cmake/onnxruntime_unittests.cmake
+++ b/cmake/onnxruntime_unittests.cmake
@@ -341,6 +341,10 @@ if(onnxruntime_USE_NGRAPH)
   list(APPEND onnxruntime_test_providers_dependencies onnxruntime_providers_ngraph)
 endif()
 
+if(onnxruntime_USE_VSI_NPU)
+  list(APPEND onnxruntime_test_providers_dependencies onnxruntime_providers_vsi_npu)
+endif()
+
 if(onnxruntime_USE_OPENVINO)
   list(APPEND onnxruntime_test_providers_dependencies onnxruntime_providers_openvino)
 endif()
@@ -399,6 +403,7 @@ set(ONNXRUNTIME_TEST_LIBS
     ${PROVIDERS_CUDA}
     # TENSORRT and DNNL are explicitly linked at runtime
     ${PROVIDERS_MIGRAPHX}
+    ${PROVIDERS_VSI_NPU}
     ${PROVIDERS_NGRAPH}
     ${PROVIDERS_OPENVINO}
     ${PROVIDERS_NUPHAR}
@@ -472,6 +477,9 @@ onnxruntime_add_include_to_target(onnxruntime_test_utils onnxruntime_common onnx
 if (onnxruntime_USE_DNNL)
   target_compile_definitions(onnxruntime_test_utils PUBLIC USE_DNNL=1)
 endif()
+if (onnxruntime_USE_VSI_NPU)
+  target_compile_definitions(onnxruntime_test_utils PUBLIC USE_VSI_NPU=1)
+endif()
 if (onnxruntime_USE_DML)
   target_add_dml(onnxruntime_test_utils)
 endif()
diff --git a/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp
index cd94ff5e5..c3bf184a4 100644
--- a/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp
+++ b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp
@@ -21,6 +21,10 @@ int main(int argc, char* argv[]) {
   // #include "cuda_provider_factory.h"
   // OrtSessionOptionsAppendExecutionProvider_CUDA(session_options, 1);
 
+  // Enable VsiNpu EP to run on GPU
+  Ort::ThrowOnError(OrtSessionOptionsAppendExecutionProvider_VsiNpu(session_options, 0));
+
+
   // Sets graph optimization level
   // Available levels are
   // ORT_DISABLE_ALL -> To disable all optimizations
diff --git a/include/onnxruntime/core/framework/tensor_shape.h b/include/onnxruntime/core/framework/tensor_shape.h
index 9a2609bc1..62e978b2e 100644
--- a/include/onnxruntime/core/framework/tensor_shape.h
+++ b/include/onnxruntime/core/framework/tensor_shape.h
@@ -54,6 +54,12 @@ class TensorShape : private std::vector<int64_t> {
   bool operator==(const TensorShape& other) const noexcept {
     auto thisVector = static_cast<const std::vector<int64_t>*>(this);
     auto otherVector = static_cast<const std::vector<int64_t>*>(&other);
+    if (thisVector->size() == 0 && otherVector->size() == 1 && (*otherVector)[0] == 1) {
+       return true;
+    }
+    if (thisVector->size() == 1 && (*thisVector)[0] == 1 && otherVector->size() == 0) {
+       return true;
+    }
     return *thisVector == *otherVector;
   }
 
diff --git a/include/onnxruntime/core/graph/constants.h b/include/onnxruntime/core/graph/constants.h
index 097665862..bb514e407 100644
--- a/include/onnxruntime/core/graph/constants.h
+++ b/include/onnxruntime/core/graph/constants.h
@@ -35,6 +35,7 @@ constexpr const char* kRknpuExecutionProvider = "RknpuExecutionProvider";
 constexpr const char* kDmlExecutionProvider = "DmlExecutionProvider";
 constexpr const char* kMIGraphXExecutionProvider = "MIGraphXExecutionProvider";
 constexpr const char* kAclExecutionProvider = "ACLExecutionProvider";
+constexpr const char* kVsiNpuExecutionProvider = "VsiNpuExecutionProvider";
 constexpr const char* kArmNNExecutionProvider = "ArmNNExecutionProvider";
 constexpr const char *providers_available[] = {
   kCpuExecutionProvider,
diff --git a/onnxruntime/core/framework/ort_value_name_idx_map.h b/onnxruntime/core/framework/ort_value_name_idx_map.h
index ed0e94402..589b848ee 100644
--- a/onnxruntime/core/framework/ort_value_name_idx_map.h
+++ b/onnxruntime/core/framework/ort_value_name_idx_map.h
@@ -35,7 +35,19 @@ class OrtValueNameIdxMap {
 
     auto it = map_.find(name);
     if (it == map_.end()) {
-      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "Could not find OrtValue with name '", name, "'");
+      std::string name1 = "";
+      for (auto kv : map_)
+      {
+        if (kv.first.find(name) != std::string::npos)
+        {
+          name1 = kv.first;
+          break;
+        }
+      }
+      it = map_.find(name1);
+      if (it == map_.end()) {
+        return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "Could not find OrtValue with name '", name, "'");
+      }
     }
 
     idx = it->second;
diff --git a/onnxruntime/core/framework/session_state.cc b/onnxruntime/core/framework/session_state.cc
index fd09448a1..9d74f91c4 100644
--- a/onnxruntime/core/framework/session_state.cc
+++ b/onnxruntime/core/framework/session_state.cc
@@ -523,7 +523,19 @@ common::Status SessionState::GetInputNodeInfo(const std::string& input_name,
                                               std::vector<NodeInfo>& node_info_vec) const {
   auto entry = input_names_to_nodeinfo_mapping_.find(input_name);
   if (entry == input_names_to_nodeinfo_mapping_.cend()) {
-    return Status(ONNXRUNTIME, FAIL, "Failed to find input name in the mapping: " + input_name);
+    std::string input_name1 = "";
+    for (auto kv : input_names_to_nodeinfo_mapping_)
+    {
+      if (kv.first.find(input_name) != std::string::npos)
+      {
+          input_name1 = kv.first;
+          break;
+      }
+    }
+    entry = input_names_to_nodeinfo_mapping_.find(input_name1);
+    if (entry == input_names_to_nodeinfo_mapping_.cend()) {
+      return Status(ONNXRUNTIME, FAIL, "Failed to find input name in the mapping: " + input_name);
+    }
   }
 
   node_info_vec = entry->second;
diff --git a/onnxruntime/core/optimizer/transformer_memcpy.cc b/onnxruntime/core/optimizer/transformer_memcpy.cc
index b0f563139..d1a7793e7 100644
--- a/onnxruntime/core/optimizer/transformer_memcpy.cc
+++ b/onnxruntime/core/optimizer/transformer_memcpy.cc
@@ -70,6 +70,7 @@ common::Status MemcpyTransformer::ApplyImpl(Graph& graph, bool& modified, int gr
         provider != onnxruntime::kOpenVINOExecutionProvider &&
         provider != onnxruntime::kNnapiExecutionProvider &&
         provider != onnxruntime::kAclExecutionProvider &&
+        provider != onnxruntime::kVsiNpuExecutionProvider &&
         provider != onnxruntime::kArmNNExecutionProvider) {
       TransformerMemcpyImpl copy_impl(graph, provider);
       auto current_modified = copy_impl.ModifyGraph(registry_manager_);
diff --git a/onnxruntime/core/session/inference_session.cc b/onnxruntime/core/session/inference_session.cc
index 218270165..c882e364c 100644
--- a/onnxruntime/core/session/inference_session.cc
+++ b/onnxruntime/core/session/inference_session.cc
@@ -1300,7 +1300,19 @@ common::Status InferenceSession::ValidateInputs(const std::vector<std::string>&
 
     auto iter = input_def_map_.find(feed_name);
     if (input_def_map_.end() == iter) {
-      return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, "Invalid Feed Input Name:", feed_name);
+      std::string feed_name1 = "";
+      for (auto kv : input_def_map_)
+      {
+        if (kv.first.find(feed_name) != std::string::npos)
+        {
+          feed_name1 = kv.first;
+          break;
+        }
+      }
+      iter = input_def_map_.find(feed_name1);
+      if (input_def_map_.end() == iter) {
+        return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, "Invalid Feed Input Name:", feed_name);
+      }
     }
 
     auto expected_type = iter->second.ml_data_type;
diff --git a/onnxruntime/test/onnx/main.cc b/onnxruntime/test/onnx/main.cc
index d3105ab41..0944107d9 100644
--- a/onnxruntime/test/onnx/main.cc
+++ b/onnxruntime/test/onnx/main.cc
@@ -37,8 +37,9 @@ void usage() {
       "\t-v: verbose\n"
       "\t-n [test_case_name]: Specifies a single test case to run.\n"
       "\t-e [EXECUTION_PROVIDER]: EXECUTION_PROVIDER could be 'cpu', 'cuda', 'dnnl', 'tensorrt', 'ngraph', "
-      "'openvino', 'nuphar', 'migraphx', 'acl' or 'armnn'. "
+      "'openvino', 'vsi_npu', 'nuphar', 'migraphx', 'acl' or 'armnn'. "
       "Default: 'cpu'.\n"
+      "\t-Q [quantize_models]: Specifies the use quantize model\n"
       "\t-p: Pause after launch, can attach debugger and continue\n"
       "\t-x: Use parallel executor, default (without -x): sequential executor.\n"
       "\t-d [device_id]: Specifies the device id for multi-device (e.g. GPU). The value should > 0\n"
@@ -96,12 +97,14 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
   bool enable_dnnl = false;
   bool enable_ngraph = false;
   bool enable_openvino = false;
+  bool enable_vsi_npu = false;
   bool enable_nuphar = false;
   bool enable_tensorrt = false;
   bool enable_mem_pattern = true;
   bool enable_nnapi = false;
   bool enable_dml = false;
   bool enable_acl = false;
+  bool enable_quantize = false;
   bool enable_armnn = false;
   bool enable_migraphx = false;
   int device_id = 0;
@@ -114,7 +117,7 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
   bool pause = false;
   {
     int ch;
-    while ((ch = getopt(argc, argv, ORT_TSTR("Ac:hj:Mn:r:e:xvo:d:p"))) != -1) {
+    while ((ch = getopt(argc, argv, ORT_TSTR("Ac:hj:Mn:r:e:xvo:d:p:Q:"))) != -1) {
       switch (ch) {
         case 'A':
           enable_cpu_mem_arena = false;
@@ -162,6 +165,8 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
             enable_ngraph = true;
           } else if (!CompareCString(optarg, ORT_TSTR("openvino"))) {
             enable_openvino = true;
+          } else if (!CompareCString(optarg, ORT_TSTR("vsi_npu"))) {
+            enable_vsi_npu = true;
           } else if (!CompareCString(optarg, ORT_TSTR("nuphar"))) {
             enable_nuphar = true;
           } else if (!CompareCString(optarg, ORT_TSTR("tensorrt"))) {
@@ -181,6 +186,9 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
             return -1;
           }
           break;
+        case 'Q':
+          enable_quantize = true;
+          break;
         case 'x':
           execution_mode = ExecutionMode::ORT_PARALLEL;
           break;
@@ -282,7 +290,13 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
 
   std::vector<std::unique_ptr<ITestCase>> owned_tests;
   {
-    double per_sample_tolerance = 1e-3;
+    double per_sample_tolerance = 0.0;
+    if(enable_quantize){
+      per_sample_tolerance = enable_vsi_npu ? 0.15 : 1e-3;
+    }else{
+      per_sample_tolerance = 1e-3;
+    }
+    //double per_sample_tolerance = enable_vsi_npu ? 0.15 : 1e-3;
     // when cuda is enabled, set it to a larger value for resolving random MNIST test failure
     // when openvino is enabled, set it to a larger value for resolving MNIST accuracy mismatch
     double relative_per_sample_tolerance = enable_cuda ? 0.017 : enable_openvino ? 0.009 : 1e-3;
@@ -329,6 +343,14 @@ int real_main(int argc, char* argv[], Ort::Env& env) {
 #else
       fprintf(stderr, "OpenVINO is not supported in this build");
       return -1;
+#endif
+    }
+    if (enable_vsi_npu) {
+#ifdef USE_VSI_NPU
+      Ort::ThrowOnError(OrtSessionOptionsAppendExecutionProvider_VsiNpu(sf, device_id));
+#else
+      fprintf(stderr, "VsiNpu is not supported in this build");
+      return -1;
 #endif
     }
     if (enable_cuda) {
diff --git a/onnxruntime/test/onnx/runner.cc b/onnxruntime/test/onnx/runner.cc
index de0de4441..a2f3db8f9 100644
--- a/onnxruntime/test/onnx/runner.cc
+++ b/onnxruntime/test/onnx/runner.cc
@@ -452,9 +452,21 @@ EXECUTE_RESULT DataRunner::RunTaskImpl(size_t task_id) {
     const std::string& output_name = output.first;
     auto iter = name_fetch_output_map.find(output_name);
     if (iter == name_fetch_output_map.end()) {
-      res = EXECUTE_RESULT::INVALID_GRAPH;
-      LOGF_DEFAULT(ERROR, "cannot find %s in the outputs", output_name.c_str());
-      break;
+      std::string output_name1 = "";
+      for (auto kv : name_fetch_output_map)
+      {
+        if (kv.first.find(output_name) != std::string::npos)
+        {
+          output_name1 = kv.first;
+          break;
+        }
+      }
+      iter = name_fetch_output_map.find(output_name1);
+      if (iter == name_fetch_output_map.end()) {
+        res = EXECUTE_RESULT::INVALID_GRAPH;
+        LOGF_DEFAULT(ERROR, "cannot find %s in the outputs", output_name.c_str());
+        break;
+      }
     }
     OrtValue* actual_output_value = iter->second;
     std::pair<COMPARE_RESULT, std::string> ret =
diff --git a/onnxruntime/test/perftest/command_args_parser.cc b/onnxruntime/test/perftest/command_args_parser.cc
index ed139249c..8f6821e55 100644
--- a/onnxruntime/test/perftest/command_args_parser.cc
+++ b/onnxruntime/test/perftest/command_args_parser.cc
@@ -98,6 +98,8 @@ namespace perftest {
           test_config.machine_config.provider_type_name = onnxruntime::kDmlExecutionProvider;
         } else if (!CompareCString(optarg, ORT_TSTR("acl"))) {
           test_config.machine_config.provider_type_name = onnxruntime::kAclExecutionProvider;
+        } else if (!CompareCString(optarg, ORT_TSTR("vsi_npu"))) {
+          test_config.machine_config.provider_type_name = onnxruntime::kVsiNpuExecutionProvider;
         } else if (!CompareCString(optarg, ORT_TSTR("armnn"))) {
           test_config.machine_config.provider_type_name = onnxruntime::kArmNNExecutionProvider;
         } else {
diff --git a/onnxruntime/test/perftest/ort_test_session.cc b/onnxruntime/test/perftest/ort_test_session.cc
index ae86b0324..2bcc8838d 100644
--- a/onnxruntime/test/perftest/ort_test_session.cc
+++ b/onnxruntime/test/perftest/ort_test_session.cc
@@ -88,6 +88,12 @@ OnnxRuntimeTestSession::OnnxRuntimeTestSession(Ort::Env& env, std::random_device
                                                      performance_test_config.run_config.enable_cpu_mem_arena ? 1 : 0));
 #else
     ORT_THROW("Acl is not supported in this build\n");
+#endif
+  } else if (provider_name == onnxruntime::kVsiNpuExecutionProvider) {
+#ifdef USE_VSI_NPU
+    Ort::ThrowOnError(OrtSessionOptionsAppendExecutionProvider_VsiNpu(session_options, 0));
+#else
+    ORT_THROW("VSI_NPU is not supported in this build\n");
 #endif
   } else if (provider_name == onnxruntime::kArmNNExecutionProvider) {
 #ifdef USE_ARMNN
diff --git a/onnxruntime/test/providers/cpu/nn/batch_norm_op_test.cc b/onnxruntime/test/providers/cpu/nn/batch_norm_op_test.cc
index 9fef174f5..834ac3b4f 100644
--- a/onnxruntime/test/providers/cpu/nn/batch_norm_op_test.cc
+++ b/onnxruntime/test/providers/cpu/nn/batch_norm_op_test.cc
@@ -46,6 +46,9 @@ void TestBatchNorm(const unordered_map<string, vector<T>>& input_data_map,
     excluded_eps.insert(kNGraphExecutionProvider);
     excluded_eps.insert(kOpenVINOExecutionProvider);
   }
+  if (expect_result == OpTester::ExpectResult::kExpectFailure) {
+    excluded_eps.insert(kVsiNpuExecutionProvider);
+  }
 
   // OpenVINO: Disabled due to software limitations
   #if defined(OPENVINO_CONFIG_GPU_FP32) || defined(OPENVINO_CONFIG_GPU_FP16) || defined(OPENVINO_CONFIG_MYRIAD) || defined(OPENVINO_CONFIG_VAD_M) || defined(OPENVINO_CONFIG_CPU_FP32)
diff --git a/onnxruntime/test/providers/provider_test_utils.cc b/onnxruntime/test/providers/provider_test_utils.cc
index 71752aaf8..b6777c11f 100644
--- a/onnxruntime/test/providers/provider_test_utils.cc
+++ b/onnxruntime/test/providers/provider_test_utils.cc
@@ -68,6 +68,44 @@ void Check(const OpTester::Data& expected_data, const Tensor& output_tensor,
   }
 }
 
+template <>
+void Check<int8_t>(const OpTester::Data& expected_data, const Tensor& output_tensor, const std::string& provider_type) {
+  auto& expected_tensor = expected_data.data_.Get<Tensor>();
+  auto* expected = expected_tensor.template Data<int8_t>();
+  auto* output = output_tensor.template Data<int8_t>();
+  auto size = output_tensor.Shape().Size();
+
+  if (expected_data.sort_output_) {
+    // if order can be jumbled in the output of an operator, sort both the expected and output buffers prior to
+    // comparison this is a "best-effort" algo and should satisfy the requirement for the few ops that do require this
+    // support without investing in a more sophisticated infrastructure for the same
+    sort_expected_and_actual_buffers<int8_t>(expected, output, size);
+  }
+
+  for (int i = 0; i < size; ++i) {
+    EXPECT_NEAR(expected[i], output[i], 1) << "i:" << i << ", provider_type: " << provider_type;
+  }
+}
+
+template <>
+void Check<uint8_t>(const OpTester::Data& expected_data, const Tensor& output_tensor, const std::string& provider_type) {
+  auto& expected_tensor = expected_data.data_.Get<Tensor>();
+  auto* expected = expected_tensor.template Data<uint8_t>();
+  auto* output = output_tensor.template Data<uint8_t>();
+  auto size = output_tensor.Shape().Size();
+
+  if (expected_data.sort_output_) {
+    // if order can be jumbled in the output of an operator, sort both the expected and output buffers prior to
+    // comparison this is a "best-effort" algo and should satisfy the requirement for the few ops that do require this
+    // support without investing in a more sophisticated infrastructure for the same
+    sort_expected_and_actual_buffers<uint8_t>(expected, output, size);
+  }
+
+  for (int i = 0; i < size; ++i) {
+    EXPECT_NEAR(expected[i], output[i], 1) << "i:" << i << ", provider_type: " << provider_type;
+  }
+}
+
 template <>
 void Check<double>(const OpTester::Data& expected_data,
                    const Tensor& output_tensor,
@@ -742,6 +780,7 @@ void OpTester::Run(
     // Run the model
     static const std::string all_provider_types[] = {
         kCpuExecutionProvider,
+        kVsiNpuExecutionProvider,
         kCudaExecutionProvider,
         kDnnlExecutionProvider,
         kNGraphExecutionProvider,
@@ -800,6 +839,8 @@ void OpTester::Run(
         std::unique_ptr<IExecutionProvider> execution_provider;
         if (provider_type == onnxruntime::kCpuExecutionProvider)
           execution_provider = DefaultCpuExecutionProvider();
+        else if (provider_type == onnxruntime::kVsiNpuExecutionProvider)
+          execution_provider = DefaultVsiNpuExecutionProvider();
         else if (provider_type == onnxruntime::kCudaExecutionProvider)
           execution_provider = DefaultCudaExecutionProvider();
         else if (provider_type == onnxruntime::kDnnlExecutionProvider)
@@ -837,6 +878,7 @@ void OpTester::Run(
               provider_type == onnxruntime::kOpenVINOExecutionProvider ||
               provider_type == onnxruntime::kTensorrtExecutionProvider ||
               provider_type == onnxruntime::kNupharExecutionProvider ||
+              provider_type == onnxruntime::kVsiNpuExecutionProvider ||
               provider_type == onnxruntime::kNnapiExecutionProvider)
             continue;
           auto reg = execution_provider->GetKernelRegistry();
diff --git a/onnxruntime/test/testdata/squeezenet/test_data_set_0/test_data_0_input.pb b/onnxruntime/test/testdata/squeezenet/test_data_set_0/input_0.pb
similarity index 100%
rename from onnxruntime/test/testdata/squeezenet/test_data_set_0/test_data_0_input.pb
rename to onnxruntime/test/testdata/squeezenet/test_data_set_0/input_0.pb
diff --git a/onnxruntime/test/testdata/squeezenet/test_data_set_0/test_data_0_output.pb b/onnxruntime/test/testdata/squeezenet/test_data_set_0/output_0.pb
similarity index 100%
rename from onnxruntime/test/testdata/squeezenet/test_data_set_0/test_data_0_output.pb
rename to onnxruntime/test/testdata/squeezenet/test_data_set_0/output_0.pb
diff --git a/onnxruntime/test/util/default_providers.cc b/onnxruntime/test/util/default_providers.cc
index 880ddd9fc..3fc298a30 100644
--- a/onnxruntime/test/util/default_providers.cc
+++ b/onnxruntime/test/util/default_providers.cc
@@ -22,6 +22,7 @@ std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_Rknpu(
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_Tensorrt(int device_id);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_MIGraphX(int device_id);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_ACL(int use_arena);
+std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_VsiNpu(int device_id);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_ArmNN(int use_arena);
 
 namespace test {
@@ -46,6 +47,14 @@ std::unique_ptr<IExecutionProvider> DefaultMIGraphXExecutionProvider() {
 #endif
 }
 
+std::unique_ptr<IExecutionProvider> DefaultVsiNpuExecutionProvider() {
+#ifdef USE_VSI_NPU
+  return CreateExecutionProviderFactory_VsiNpu(0)->CreateProvider();
+#else
+  return nullptr;
+#endif
+}
+
 std::unique_ptr<IExecutionProvider> DefaultOpenVINOExecutionProvider() {
 #ifdef USE_OPENVINO
   return CreateExecutionProviderFactory_OpenVINO("", false, "")->CreateProvider();
diff --git a/onnxruntime/test/util/include/default_providers.h b/onnxruntime/test/util/include/default_providers.h
index 21465d746..c0ae1d527 100644
--- a/onnxruntime/test/util/include/default_providers.h
+++ b/onnxruntime/test/util/include/default_providers.h
@@ -14,6 +14,7 @@ std::unique_ptr<IExecutionProvider> DefaultNGraphExecutionProvider();
 std::unique_ptr<IExecutionProvider> DefaultNupharExecutionProvider(bool allow_unaligned_buffers = true);
 std::unique_ptr<IExecutionProvider> DefaultTensorrtExecutionProvider();
 std::unique_ptr<IExecutionProvider> DefaultMIGraphXExecutionProvider();
+std::unique_ptr<IExecutionProvider> DefaultVsiNpuExecutionProvider();
 std::unique_ptr<IExecutionProvider> DefaultOpenVINOExecutionProvider();
 std::unique_ptr<IExecutionProvider> DefaultNnapiExecutionProvider();
 std::unique_ptr<IExecutionProvider> DefaultRknpuExecutionProvider();
diff --git a/onnxruntime/test/util/include/providers.h b/onnxruntime/test/util/include/providers.h
index fc0147958..eabf6700c 100644
--- a/onnxruntime/test/util/include/providers.h
+++ b/onnxruntime/test/util/include/providers.h
@@ -22,6 +22,9 @@
 #ifdef USE_OPENVINO
 #include "core/providers/openvino/openvino_provider_factory.h"
 #endif
+#ifdef USE_VSI_NPU
+#include "core/providers/vsi_npu/vsi_npu_provider_factory.h"
+#endif
 #ifdef USE_NNAPI
 #include "core/providers/nnapi/nnapi_provider_factory.h"
 #endif
diff --git a/tools/ci_build/build.py b/tools/ci_build/build.py
index a2629ff63..425389d8c 100755
--- a/tools/ci_build/build.py
+++ b/tools/ci_build/build.py
@@ -267,6 +267,8 @@ def parse_arguments():
     parser.add_argument("--eigen_path", help="Path to pre-installed Eigen.")
     parser.add_argument(
         "--use_openmp", action='store_true', help="Build with OpenMP")
+    parser.add_argument(
+        "--use_vsi_npu", action='store_true', help="Build with VSI NPU.")
     parser.add_argument(
         "--enable_msinternal", action="store_true",
         help="Enable for Microsoft internal builds only.")
@@ -321,6 +323,10 @@ def parse_arguments():
     parser.add_argument(
         "--use_telemetry", action='store_true',
         help="Only official builds can set this flag to enable telemetry.")
+    parser.add_argument(
+        "--use_cross_compile", action='store_true', help="Use corss compile.")
+    parser.add_argument(
+        "--cmake_toolchain", help="Path to cmake tool chain.")
     parser.add_argument(
         "--enable_wcos", action='store_true',
         help="Build for Windows Core OS.")
@@ -404,7 +410,8 @@ def get_linux_distro():
 
 def is_ubuntu_1604():
     dist, ver = get_linux_distro()
-    return dist == 'Ubuntu' and ver.startswith('16.04')
+    # return dist == 'Ubuntu' and ver.startswith('16.04')
+    return True
 
 
 def get_config_build_dir(build_dir, config):
@@ -495,8 +502,8 @@ def install_python_deps(numpy_version=""):
     dep_packages.append('sympy>=1.1')
     dep_packages.append('packaging')
     dep_packages.append('cerberus')
-    run_subprocess([sys.executable, '-m', 'pip', 'install', '--trusted-host',
-                    'files.pythonhosted.org'] + dep_packages)
+    # run_subprocess([sys.executable, '-m', 'pip', 'install', '--trusted-host',
+    #                 'files.pythonhosted.org'] + dep_packages)
 
 
 # We need to install Torch to test certain functionalities of the ORT Python package
@@ -608,6 +615,7 @@ def generate_build_tree(cmake_path, source_dir, build_dir, cuda_home, cudnn_home
         "-Donnxruntime_USE_NGRAPH=" + ("ON" if args.use_ngraph else "OFF"),
         "-Donnxruntime_USE_NNAPI_BUILTIN=" + ("ON" if args.use_nnapi else "OFF"),
         "-Donnxruntime_USE_RKNPU=" + ("ON" if args.use_rknpu else "OFF"),
+        "-Donnxruntime_USE_VSI_NPU=" + ("ON" if args.use_vsi_npu else "OFF"),
         "-Donnxruntime_USE_OPENMP=" + (
             "ON" if args.use_openmp and not (
                 args.use_nnapi or (args.use_mklml and (is_macOS() or is_windows())) or args.use_ngraph or
@@ -722,6 +730,9 @@ def generate_build_tree(cmake_path, source_dir, build_dir, cuda_home, cudnn_home
         nvml_stub_path = cuda_home + "/lib64/stubs"
         cmake_args += ["-DCUDA_CUDA_LIBRARY=" + nvml_stub_path]
 
+    if args.use_cross_compile:
+        cmake_args += ["-DCMAKE_TOOLCHAIN_FILE=" + args.cmake_toolchain]
+
     if args.use_preinstalled_eigen:
         cmake_args += ["-Donnxruntime_USE_PREINSTALLED_EIGEN=ON",
                        "-Deigen_SOURCE_PATH=" + args.eigen_path]
@@ -930,7 +941,8 @@ def build_targets(args, cmake_path, build_dir, configs, parallel, target=None):
                 # CMake will generate correct build tool args for Xcode
                 cmd_args += ["--parallel", num_cores]
             elif args.cmake_generator != 'Ninja':
-                build_tool_args += ["-j" + num_cores]
+                num = os.getenv('BUILD_THREAD', default=num_cores)
+                build_tool_args += ["-j" + num]
 
         if build_tool_args:
             cmd_args += ["--"]
@@ -1448,10 +1460,10 @@ def run_csharp_tests(use_cuda, use_openvino, use_tensorrt, use_dnnl):
 
 
 def build_protoc_for_host(cmake_path, source_dir, build_dir, args):
-    if (args.arm or args.arm64 or args.enable_windows_store) and (not is_windows() and not args.ios):
-        raise BuildError(
-            'Currently only support building protoc for Windows host while '
-            'cross-compiling for ARM/ARM64/Store and linux cross-compiling iOS')
+    # if (args.arm or args.arm64 or args.enable_windows_store) and (not is_windows() and not args.ios):
+    #     raise BuildError(
+    #         'Currently only support building protoc for Windows host while '
+    #         'cross-compiling for ARM/ARM64/Store and linux cross-compiling iOS')
 
     log.info(
         "Building protoc for host to be used in cross-compiled build process")
@@ -1707,9 +1719,8 @@ def main():
 
         if is_ubuntu_1604():
             if (args.arm or args.arm64):
-                raise BuildError(
-                    "Only Windows ARM(64) cross-compiled builds supported "
-                    "currently through this script")
+                path_to_protoc_exe = build_protoc_for_host(
+                    cmake_path, source_dir, build_dir, args)
             install_ubuntu_deps(args)
             if not is_docker() and not args.use_acl and not args.use_armnn:
                 install_python_deps()
-- 
2.25.1

